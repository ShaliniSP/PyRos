{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train  = pd.read_csv(\"C:/Users/salon/Desktop/Fifth Semester/Data Analytics/Project Datasets/Colaborative Filtering/Colab/collab_train.csv\")\n",
    "df_test  = pd.read_csv(\"C:/Users/salon/Desktop/Fifth Semester/Data Analytics/Project Datasets/Colaborative Filtering/Colab/collab_test.csv\")\n",
    "df_train  = df_train[['gameid','username','rating']]\n",
    "df_test = df_test[['gameid','username','rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gameid</th>\n",
       "      <th>username</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104347</td>\n",
       "      <td>Larry Chong</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25613</td>\n",
       "      <td>bluekingzog</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35497</td>\n",
       "      <td>Bundyman</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>laycelin</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20551</td>\n",
       "      <td>gixmo</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gameid     username  rating\n",
       "0  104347  Larry Chong     7.0\n",
       "1   25613  bluekingzog    10.0\n",
       "2   35497     Bundyman     6.5\n",
       "3      47     laycelin     8.0\n",
       "4   20551        gixmo     3.0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(879, 3)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gameid</th>\n",
       "      <th>username</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160477</td>\n",
       "      <td>549sd</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>171669</td>\n",
       "      <td>aaronseeber</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12942</td>\n",
       "      <td>agentzen</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5867</td>\n",
       "      <td>Aiken Drum</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>164928</td>\n",
       "      <td>Aiken Drum</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gameid     username  rating\n",
       "0  160477        549sd     8.0\n",
       "1  171669  aaronseeber     7.0\n",
       "2   12942     agentzen     4.0\n",
       "3    5867   Aiken Drum     6.0\n",
       "4  164928   Aiken Drum     6.0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 3)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of train matrix:  0.9992415623814941\n",
      "Sparsity of test matrix:  0.996969696969697\n"
     ]
    }
   ],
   "source": [
    "#Indexed values for gameid and username fields\n",
    "\n",
    "trainrows = df_train.username.unique()\n",
    "traincols = df_train['gameid'].unique()\n",
    "df_train = df_train[['gameid', 'username', 'rating']]\n",
    "df_train\n",
    "idict = dict(zip(traincols, range(len(traincols))))\n",
    "udict = dict(zip(trainrows, range(len(trainrows))))\n",
    "idict\n",
    "udict\n",
    "df_train.username = [ udict[i] for i in df_train.username ]\n",
    "df_train['gameid'] = [ idict[i] for i in df_train['gameid'] ]\n",
    "trainmat = df_train.values\n",
    "testrows = df_test.username.unique()\n",
    "testcols = df_test['gameid'].unique()\n",
    "df_test = df_test[['gameid', 'username', 'rating']]\n",
    "idict = dict(zip(testcols, range(len(testcols))))\n",
    "udict = dict(zip(testrows, range(len(testrows))))\n",
    "df_test.username = [ udict[i] for i in df_test.username ]\n",
    "df_test['gameid'] = [ idict[i] for i in df_test['gameid'] ]\n",
    "testmat = df_test.values\n",
    "\n",
    "def nonzerocount(a):\n",
    "    count = 0\n",
    "    for i in a:\n",
    "        for j in i:\n",
    "            if j != 0:\n",
    "                count += 1\n",
    "    return count\n",
    "print(\"Sparsity of train matrix: \",nonzerocount(trainmat)/np.prod(trainmat.shape))\n",
    "print(\"Sparsity of test matrix: \",nonzerocount(testmat)/np.prod(testmat.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique users in df_train 617\n",
      "Unique games in df_train 617\n",
      "Unique users in df_test 201\n",
      "Unique games in df_test 198\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique users in df_train\", len(trainrows))\n",
    "print(\"Unique games in df_train\", len(traincols))\n",
    "print(\"Unique users in df_test\", len(testrows))\n",
    "print(\"Unique games in df_test\", len(testcols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 617)\n"
     ]
    }
   ],
   "source": [
    "naive_train = np.zeros((len(trainrows),len(traincols))) #Pivot table for train dataset\n",
    "for row in trainmat:\n",
    "  naive_train[int(row[1]), int(row[0])] = int(row[2])\n",
    "print(naive_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201, 198)\n"
     ]
    }
   ],
   "source": [
    "naive_test = np.zeros((len(testrows),len(testcols)))#Pivot table for test dataset\n",
    "for row in testmat:\n",
    "  naive_test[int(row[1]), int(row[0])] = int(row[2])\n",
    "print(naive_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 617)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "\n",
    "similarities = cosine_similarity(naive_train) #Calculating the user-user similairty matrix\n",
    "print(similarities.shape)\n",
    "\n",
    "#Finding k similar users\n",
    "def ksimilar(user, SM, k):\n",
    "    similarities = SM[user]\n",
    "    similarities[user] = 0\n",
    "    similar_users = []\n",
    "    for i in range(k):\n",
    "        m = np.argmax(similarities)\n",
    "        similar_users.append(m)\n",
    "        similarities[m] = 0\n",
    "    return similar_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.01134522 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.02755267 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.00972447 0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.00972447 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.01458671 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.01134522 0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "prediction_train = np.zeros_like(naive_train)\n",
    "real_train = np.zeros_like(naive_train)\n",
    "\n",
    "for row, val in enumerate(naive_train):\n",
    "  avg_rating = np.mean(naive_train[row]) #On an average how has the user rated games\n",
    "  sim = similarities[row]  #Stores all the similarity values of that user with respect to other users\n",
    "  for col in val:\n",
    "    prediction_train[int(row), int(col)] = avg_rating + np.sum(sim*naive_train[int(row)][int(col)])/np.sum(np.abs(sim))\n",
    "    real_train[int(row), int(col)] = naive_train[int(row)][int(col)]\n",
    "\n",
    "print(prediction_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 617)\n"
     ]
    }
   ],
   "source": [
    "print(prediction_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.04040404 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.03535354 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.02020202 0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.03535354 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.03535354 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.03535354 0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "prediction_test = np.zeros_like(naive_test)\n",
    "real_test = np.zeros_like(naive_test)\n",
    "\n",
    "for row, val in enumerate(naive_test):\n",
    "  avg_rating = np.mean(naive_test[row]) #On an average how has the user rated games\n",
    "  sim = similarities[row]  #Stores all the similarity values of that user with respect to other users\n",
    "  for col in val:\n",
    "    prediction_test[int(row), int(col)] = avg_rating + np.sum(sim*naive_train[int(row)][int(col)])/np.sum(np.abs(sim))\n",
    "    real_test[int(row), int(col)] = naive_test[int(row)][int(col)]\n",
    "\n",
    "print(prediction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201, 198)\n"
     ]
    }
   ],
   "source": [
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math #Defining the rmse function\n",
    "def rmse(a, b):\n",
    "    return math.sqrt(np.square(a - b).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error\n",
      "RMSE : 0.0012\n"
     ]
    }
   ],
   "source": [
    "err1 = rmse(np.array(prediction_train), np.array(real_train))\n",
    "print('Train Error')\n",
    "print('RMSE : %.4f' % err1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error\n",
      "RMSE : 0.0713\n"
     ]
    }
   ],
   "source": [
    "err2 = rmse(np.array(prediction_test), np.array(real_test))\n",
    "print('Test Error')\n",
    "print('RMSE : %.4f' % err2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
